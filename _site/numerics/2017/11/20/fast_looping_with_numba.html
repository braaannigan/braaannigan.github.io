<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Fast looping with Numba | Liam Brannigan</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Fast looping with Numba" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Data Scientist" />
<meta property="og:description" content="Data Scientist" />
<link rel="canonical" href="http://0.0.0.0:4000/numerics/2017/11/20/fast_looping_with_numba.html" />
<meta property="og:url" content="http://0.0.0.0:4000/numerics/2017/11/20/fast_looping_with_numba.html" />
<meta property="og:site_name" content="Liam Brannigan" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-11-20T02:35:24-06:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Fast looping with Numba" />
<script type="application/ld+json">
{"url":"http://0.0.0.0:4000/numerics/2017/11/20/fast_looping_with_numba.html","@type":"BlogPosting","headline":"Fast looping with Numba","dateModified":"2017-11-20T02:35:24-06:00","datePublished":"2017-11-20T02:35:24-06:00","description":"Data Scientist","mainEntityOfPage":{"@type":"WebPage","@id":"http://0.0.0.0:4000/numerics/2017/11/20/fast_looping_with_numba.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=7a18c2c95540c5e25cb727688cbebc4ca453676a">
  </head>
  <body>
    <div class="container-lg px-3 my-5 markdown-body">
      
      <h1><a href="http://0.0.0.0:4000/">Liam Brannigan</a></h1>
      

      <script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<p>tl;dr Just-In-Time compilers can give significant speed-ups over vectorised calculations, but you should be aware of potential pitfalls with this rapidly developing technology.</p>

<h2 id="thou-shalt-not-loop-in-python-most-of-the-time">Thou shalt not loop in python, most of the time</h2>
<p>One of the most useful things you can learn as you begin as a
scientific programmer in python
is to avoid writing loops to perform calculations.  These loops are known to
be <strong>so much slower</strong> than equivalent <em>vectorised</em> calculations.  For example,
compare this simple sum of two matrices as a loop versus a vectorised
operation using the + operator.</p>

<p>First we define two input arrays:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1000000</span><span class="p">)</span>
<span class="n">data2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1000000</span><span class="p">)</span>
</code></pre></div></div>
<p>We can define a looping function as:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">loop_sum</span><span class="p">(</span> <span class="n">data</span><span class="p">,</span> <span class="n">data2</span> <span class="p">):</span>
    <span class="n">sum_array</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">empty_like</span><span class="p">(</span> <span class="n">data</span> <span class="p">)</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span> <span class="nb">len</span><span class="p">(</span> <span class="n">data</span> <span class="p">)</span> <span class="p">):</span>
        <span class="n">sum_array</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">data2</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">sum_array</span>
</code></pre></div></div>
<p>while a vectorised addition function would be:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">vectorised_sum</span><span class="p">(</span> <span class="n">data</span><span class="p">,</span> <span class="n">data2</span> <span class="p">):</span>
    <span class="k">return</span> <span class="n">data</span> <span class="o">+</span> <span class="n">data2</span>
</code></pre></div></div>
<p>To run the loop on my machine it takes:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">timeit</span> <span class="n">loop_sum</span><span class="p">(</span> <span class="n">data</span><span class="p">,</span> <span class="n">data2</span> <span class="p">)</span>
<span class="mi">348</span> <span class="n">ms</span> <span class="err">±</span> <span class="mf">3.21</span> <span class="n">ms</span> <span class="n">per</span> <span class="n">loop</span> <span class="p">(</span><span class="n">mean</span> <span class="err">±</span> <span class="n">std</span><span class="p">.</span> <span class="n">dev</span><span class="p">.</span> <span class="n">of</span> <span class="mi">7</span> <span class="n">runs</span><span class="p">,</span> <span class="mi">1</span> <span class="n">loop</span> <span class="n">each</span><span class="p">)</span>
</code></pre></div></div>
<p>while the vectorised function takes:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">timeit</span> <span class="n">vectorised_sum</span><span class="p">(</span> <span class="n">data</span><span class="p">,</span> <span class="n">data2</span> <span class="p">)</span>
<span class="mf">1.11</span> <span class="n">ms</span> <span class="err">±</span> <span class="mf">51.9</span> <span class="n">µs</span> <span class="n">per</span> <span class="n">loop</span> <span class="p">(</span><span class="n">mean</span> <span class="err">±</span> <span class="n">std</span><span class="p">.</span> <span class="n">dev</span><span class="p">.</span> <span class="n">of</span> <span class="mi">7</span> <span class="n">runs</span><span class="p">,</span> <span class="mi">1000</span> <span class="n">loops</span> <span class="n">each</span><span class="p">)</span>
</code></pre></div></div>
<p>In this case the vectorised calculation is about 300 times faster.</p>

<p>The reason for this faster performance <a href="http://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/">has been explained before:</a>
python needs to do a lot less type checking when working with arrays rather than individual values. In addition, the data in an array is always stored in a contiguous block in memory, whereas the underlying data in other structures such as lists might be scatted around the memory.</p>

<h2 id="is-there-a-role-for-looping-then">Is there a role for looping then?</h2>
<p>One problem with this focus on vectorised operations is that when the calculation is complicated, it is often easier to write loops than the equivalent
vectorised operation.  Indeed, when I’m figuring out a calculation on
a grid, I often work it out on paper in a loop format and then have to
convert this loop to a vectorised operation.  This extra step can be a problem, as
doing the vectorisation often gives me a headache when figuring out
edge cases such as dealing with coastlines in numerical models.</p>

<p>One way to stick with the looping option is to use the
<a href="http://Numba.pydata.org">Numba</a> package. You then write your loop
function as before, but this time add the <em>decorator</em> <code class="language-plaintext highlighter-rouge">@jit</code> to the start of your function:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">Numba</span> <span class="kn">import</span> <span class="n">jit</span>

<span class="o">@</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">numba_loop_sum</span><span class="p">(</span> <span class="n">data</span><span class="p">,</span> <span class="n">data2</span> <span class="p">):</span>
    <span class="n">sum_array</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">empty_like</span><span class="p">(</span> <span class="n">data</span> <span class="p">)</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span> <span class="nb">len</span><span class="p">(</span> <span class="n">data</span> <span class="p">)</span> <span class="p">):</span>
        <span class="n">sum_array</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">data2</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">sum_array</span>
</code></pre></div></div>
<p>By adding <code class="language-plaintext highlighter-rouge">@jit</code> as a decorator your function
is “Just-In-Time” compiled into fast machine code when you call the function. We can compare the performance of this operation with Numba to the vectorised function:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">timeit</span> <span class="n">numba_loop_sum</span><span class="p">(</span> <span class="n">data</span><span class="p">,</span> <span class="n">data2</span> <span class="p">)</span>
<span class="mf">3.42</span> <span class="n">ms</span> <span class="err">±</span> <span class="mf">20.2</span> <span class="n">µs</span> <span class="n">per</span> <span class="n">loop</span> <span class="p">(</span><span class="n">mean</span> <span class="err">±</span> <span class="n">std</span><span class="p">.</span> <span class="n">dev</span><span class="p">.</span> <span class="n">of</span> <span class="mi">7</span> <span class="n">runs</span><span class="p">,</span> <span class="mi">1</span> <span class="n">loop</span> <span class="n">each</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="but-wasnt-that-slower-than-the-vectorised-calculation">But wasn’t that slower than the vectorised calculation?</h2>
<p>Ok, so the numba calculation was a lot slower in this case, so it may be worth the extra work to do the vectorisation.  However, let’s compare a more complicated case that is more similar to the problem I was working on this week.</p>

<p>The vectorised version is:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">complicated_vectorised_calc</span><span class="p">(</span> <span class="n">data</span><span class="p">,</span> <span class="n">data2</span> <span class="p">):</span>
    <span class="n">total</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">data2</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span> <span class="o">+</span> <span class="n">data</span><span class="o">**</span><span class="mi">3</span> <span class="o">+</span> <span class="n">data2</span><span class="o">**</span><span class="mi">4</span>
    <span class="k">return</span> <span class="n">total</span>
</code></pre></div></div>
<p>while the corresponding Numba loop is:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">Numba_loop_calc</span><span class="p">(</span> <span class="n">data</span><span class="p">,</span> <span class="n">data2</span> <span class="p">):</span>
    <span class="n">total</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">empty_like</span><span class="p">(</span> <span class="n">data</span> <span class="p">)</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span> <span class="nb">len</span><span class="p">(</span> <span class="n">data</span> <span class="p">)</span> <span class="p">):</span>
        <span class="n">total</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">data2</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span> <span class="o">+</span> <span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">**</span><span class="mi">3</span> <span class="o">+</span> <span class="n">data2</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">**</span><span class="mi">4</span>
    <span class="k">return</span> <span class="n">total</span>
</code></pre></div></div>
<p>Comparing the performance for this more complicated case, we find that for the vectorised calculation it takes about 122 ms per calculation, while the Numba calculation takes 4 ms. So in this case Numba gives a speed up of 30 times.</p>

<h2 id="whats-going-on-with-that">What’s going on with that?</h2>
<p>A major selling point of NumPy is that it compiles and carries out your calculations in C. As such, you might expect it to offer similar performance to a compiled function decorated with <code class="language-plaintext highlighter-rouge">@jit</code>, rather than being 30 times slower.</p>

<p>The reason for this slowdown, is that NumPy may not be doing exactly when you think it is doing in the compiled layer.  For the complicated calculation, you might guess than NumPy is doing something like the loop in <code class="language-plaintext highlighter-rouge">Numba_loop_calc</code> with all the type declarations and so on that you would expect in C.</p>

<p>However, NumPy is actually doing a series of sub-loops for each operation in the overall loop and this sub-looping introduces extra computational overhead.  NumPy also needs to create at least one temporary array to hold the output of the sub-loops that have already been done, and so NumPy requires more memory and more calls to memory.  This is all explained by Nathaniel Smith (a person who knows a lot more about NumPy than I do!) in the video below. The whole talk is worth watching, but the part setting out the part on JIT compilers starts at 26 minutes:</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/fowHwlpGb34?rel=0" frameborder="0" allowfullscreen=""></iframe>

<p>What all this means is that as your calculation becomes more complicated, the attraction of writing a loop decorated with <code class="language-plaintext highlighter-rouge">@jit</code> increases. The JIT function is particularly attractive  if the creation of temporary arrays means that your calculation is causing memory errors.</p>

<h2 id="great-so-theres-no-downsides">Great, so there’s no downsides.</h2>
<p>Unfortunately, there is a catch.  The main one is that the automatic compilation built into Numba is very complicated, so for most users it’s not going to be very clear what is happening or whether something is silently going wrong.  If you do use Numba, then it might be a good time to <a href="http://swcarpentry.github.io/python-novice-inflammation/08-defensive/">write a test function</a> to make sure your output is what you expect.</p>

<p>A further issue is that the tech in automatic compilation is still rapidly maturing.  It would be reasonable to expect Numba to continue evolving over the coming years.  This can mean that the API changes or even that changes to the underlying algorithm mean the output you get changes. On account of this, I’d be wary of building Numba into a widely-used library unless you can be sure it will continue to be supported.</p>

<p>The output of the NumPy function and the Numba function also differ for some values in the output arrays on my machine.  These differences are small (order 1e-16), but nonetheless exist.</p>

<h2 id="so-where-are-we-headed">So, where are we headed?</h2>
<p>The rapid development in Just-In-Time compilation has raised an interesting question discussed by Nathaniel Smith in the above video: should NumPy be developed to take advantage of this capability?</p>

<p>The problem is that the JIT compilers can’t see down to the C library that underlies NumPy. Instead, Numba works with NumPy because the developers of Numba have gone through each of the NumPy functions that Numba supports and developed compiled code for the functions.  This isn’t a long-term solution for many reasons.  For example, every time the underlying NumPy function changes, someone will have to make sure Numba is changed accordingly.  It also means you’d have to start from scratch if you want to start using a different kernel such as PyTorch or TensorFlow.</p>

<p>In the meantime, it’s good to be aware of the strengths and weaknesses of JIT compilers for optimising your code.</p>


      
      <div class="footer border-top border-gray-light mt-5 pt-3 text-right text-gray">
        This site is open source. <a href="https://github.com/braaannigan/braaannigan.github.io/edit/gh-pages/_posts/2017-11-20-fast_looping_with_numba.markdown">Improve this page</a>.
      </div>
      
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.0/anchor.min.js" integrity="sha256-lZaRhKri35AyJSypXXs4o6OPFTbTmUoltBbDCbdzegg=" crossorigin="anonymous"></script>
    <script>anchors.add();</script>
    
  </body>
</html>
